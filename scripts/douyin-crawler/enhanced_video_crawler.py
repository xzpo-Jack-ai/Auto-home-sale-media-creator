#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæŠ–éŸ³è§†é¢‘æŠ“å– - æå–æ›´å¤šå­—æ®µ
åŒ…æ‹¬ï¼šæ’­æ”¾é‡ã€å‘å¸ƒæ—¶é—´ã€è§†é¢‘é“¾æ¥ã€å°é¢å›¾ç­‰
"""

import asyncio
import json
import re
import sqlite3
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional
from urllib.parse import quote
from playwright.async_api import async_playwright, Page

COOKIE_FILE = Path(__file__).parent / "cookies.json"
DB_PATH = Path("/Volumes/movespace/workspace/Auto-home-sale-media-creator/apps/api/dev.db")
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)

CITIES = ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'å¹¿å·', 'æ­å·', 'æˆéƒ½']

@dataclass
class VideoData:
    city: str
    title: str
    author: str
    author_id: str
    views: int
    likes: int
    shares: int
    comments: int
    video_url: str
    cover_url: str
    duration: int
    published_at: Optional[datetime]
    crawled_at: datetime


class EnhancedVideoCrawler:
    def __init__(self):
        self.cookies = json.load(open(COOKIE_FILE))
        self.results: List[VideoData] = []
    
    async def init(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=False)
        self.context = await self.browser.new_context(
            viewport={'width': 1400, 'height': 900},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        await self.context.add_cookies(self.cookies)
        print("âœ… æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        await self.context.close()
        await self.browser.close()
        await self.playwright.stop()
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—ï¼ˆæ”¯æŒä¸‡ã€äº¿ç­‰å•ä½ï¼‰"""
        if not text:
            return 0
        text = str(text).replace(',', '').strip()
        match = re.search(r'(\d+(?:\.\d+)?)\s*[ä¸‡äº¿]?', text)
        if match:
            num = float(match.group(1))
            if 'ä¸‡' in text:
                return int(num * 10000)
            elif 'äº¿' in text:
                return int(num * 100000000)
            return int(num)
        return 0
    
    def _parse_time(self, text: str) -> Optional[datetime]:
        """è§£ææ—¶é—´æ–‡æœ¬"""
        if not text:
            return None
        
        # åŒ¹é… "2026-02-25" æˆ– "02-25"
        patterns = [
            (r'(\d{4}-\d{2}-\d{2})', '%Y-%m-%d'),
            (r'(\d{2}-\d{2})', '%m-%d'),
        ]
        
        for pattern, fmt in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    date_str = match.group(1)
                    if fmt == '%m-%d':
                        date_str = f"2026-{date_str}"
                    return datetime.strptime(date_str, '%Y-%m-%d')
                except:
                    pass
        
        # å¤„ç†"Xå¤©å‰"
        day_match = re.search(r'(\d+)\s*å¤©å‰', text)
        if day_match:
            days = int(day_match.group(1))
            return datetime.now() - timedelta(days=days)
        
        return None
    
    async def crawl_city(self, city: str) -> List[VideoData]:
        """æŠ“å–å•ä¸ªåŸå¸‚"""
        videos = []
        
        try:
            page = await self.context.new_page()
            
            # ç›´æ¥è®¿é—®è§†é¢‘æœç´¢URL
            query = f"{city},æˆ¿äº§"
            url = f"https://creator.douyin.com/creator-micro/creator-count/arithmetic-index/videosearch?query={quote(query)}&source=creator&page=1"
            
            print(f"\nğŸ“ [{city}] è®¿é—®è§†é¢‘æœç´¢...")
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await asyncio.sleep(5)
            
            # å…³é—­å¼¹çª—
            try:
                btn = await page.wait_for_selector('button:has-text("ç¡®è®¤")', timeout=3000)
                if btn:
                    await btn.click()
                    await asyncio.sleep(2)
                    print(f"   âœ… å·²å…³é—­å¼¹çª—")
            except:
                pass
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            for _ in range(3):
                await page.evaluate('window.scrollBy(0, 500)')
                await asyncio.sleep(1)
            
            # ä½¿ç”¨JavaScriptæå–å®Œæ•´çš„è§†é¢‘ä¿¡æ¯
            video_data = await page.evaluate('''() => {
                const results = [];
                const seen = new Set();
                
                // æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«è§†é¢‘ä¿¡æ¯çš„å®¹å™¨
                const containers = document.querySelectorAll('div[class*="card"], div[class*="item"], [class*="video"]');
                
                for (const container of containers) {
                    // æŸ¥æ‰¾æ ‡é¢˜ï¼ˆåŒ…å«#è¯é¢˜æ ‡ç­¾çš„é•¿æ–‡æœ¬ï¼‰
                    const titleEl = container.querySelector('span, div, h3, h4, a');
                    if (!titleEl) continue;
                    
                    const titleText = titleEl.textContent.trim();
                    
                    // ç­›é€‰æ¡ä»¶
                    if (titleText.length < 30 || titleText.length > 150 || 
                        !titleText.includes('#') ||
                        titleText.includes('é¦–é¡µ') ||
                        titleText.includes('å†…å®¹ç®¡ç†') ||
                        titleText.includes('æ•°æ®ä¸­å¿ƒ')) {
                        continue;
                    }
                    
                    // å»é‡
                    if (seen.has(titleText)) continue;
                    seen.add(titleText);
                    
                    // æå–ä½œè€…
                    let author = 'æœªçŸ¥ä½œè€…';
                    const allText = container.textContent;
                    const lines = allText.split('\\n').map(l => l.trim()).filter(l => l);
                    for (const line of lines) {
                        if (line.length > 2 && line.length < 20 && 
                            !line.includes('#') && 
                            !line.includes('å‘å¸ƒäº') &&
                            !line.match(/^\\d/)) {
                            author = line;
                            break;
                        }
                    }
                    
                    // æå–æ’­æ”¾é‡/çƒ­åº¦ï¼ˆæŸ¥æ‰¾åŒ…å«æ•°å­—å’Œ"ä¸‡"çš„æ–‡æœ¬ï¼‰
                    let views = '';
                    const viewMatch = allText.match(/(\\d+(?:\\.\\d+)?)[ä¸‡]?[^\\d]*(?:æ’­æ”¾|çƒ­åº¦|æŒ‡æ•°)/);
                    if (viewMatch) views = viewMatch[0];
                    
                    // æå–å‘å¸ƒæ—¶é—´
                    let publishTime = '';
                    const timeMatch = allText.match(/(\\d{4}-\\d{2}-\\d{2})/);
                    if (timeMatch) publishTime = timeMatch[0];
                    
                    // æå–è§†é¢‘æ—¶é•¿ï¼ˆå¦‚ 03:01ï¼‰
                    let duration = '';
                    const durationMatch = allText.match(/(\\d{1,2}:\\d{2})/);
                    if (durationMatch) duration = durationMatch[0];
                    
                    // æå–è§†é¢‘é“¾æ¥
                    let videoUrl = '';
                    const linkEl = container.querySelector('a[href*="/video/"], a[href*="/share/"]');
                    if (linkEl) {
                        videoUrl = linkEl.href;
                    }
                    
                    // æå–å°é¢å›¾
                    let coverUrl = '';
                    const imgEl = container.querySelector('img');
                    if (imgEl) {
                        coverUrl = imgEl.src;
                    }
                    
                    results.push({
                        title: titleText,
                        author,
                        views,
                        publishTime,
                        duration,
                        videoUrl,
                        coverUrl
                    });
                    
                    if (results.length >= 10) break;
                }
                
                return results;
            }''')
            
            print(f"   âœ… æå–åˆ° {len(video_data)} ä¸ªè§†é¢‘")
            
            for data in video_data:
                # è§£ææ—¶é•¿ï¼ˆç§’ï¼‰
                duration_sec = 0
                if data.get('duration'):
                    parts = data['duration'].split(':')
                    if len(parts) == 2:
                        duration_sec = int(parts[0]) * 60 + int(parts[1])
                
                videos.append(VideoData(
                    city=city,
                    title=data['title'][:200],
                    author=data['author'][:50],
                    author_id='',
                    views=self._parse_number(data.get('views', '')),
                    likes=0,
                    shares=0,
                    comments=0,
                    video_url=data.get('videoUrl', '')[:500],
                    cover_url=data.get('coverUrl', '')[:500],
                    duration=duration_sec,
                    published_at=self._parse_time(data.get('publishTime', '')),
                    crawled_at=datetime.now()
                ))
                print(f"      âœ“ {data['title'][:40]}... | {data.get('author', 'N/A')} | {data.get('views', 'N/A')}")
            
            await page.close()
            
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {str(e)[:80]}")
        
        return videos
    
    def save_to_db(self, videos: List[VideoData]):
        if not DB_PATH.exists() or not videos:
            return
        
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            saved = 0
            for v in videos:
                external_id = f"enh_{v.city}_{hash(v.title) % 1000000}_{int(datetime.now().timestamp())}"
                
                cursor.execute('''
                    INSERT OR REPLACE INTO videos (
                        id, externalId, platform, title, author, authorId,
                        views, likes, shares, comments, coverUrl, videoUrl, duration,
                        transcript, publishedAt, keyword, city, createdAt, updatedAt
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    external_id, external_id, 'douyin', v.title, v.author, v.author_id,
                    v.views, v.likes, v.shares, v.comments, v.cover_url, v.video_url, v.duration,
                    '',
                    v.published_at.strftime('%Y-%m-%d %H:%M:%S') if v.published_at else None,
                    f"{v.city}æˆ¿äº§", v.city,
                    v.crawled_at.strftime('%Y-%m-%d %H:%M:%S'),
                    v.crawled_at.strftime('%Y-%m-%d %H:%M:%S')
                ))
                saved += 1
            
            conn.commit()
            conn.close()
            print(f"ğŸ’¾ ä¿å­˜ {saved} æ¡åˆ°æ•°æ®åº“")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
    
    async def run(self):
        print("=" * 70)
        print("ğŸš€ å¢å¼ºç‰ˆæŠ–éŸ³è§†é¢‘æŠ“å– - æå–æ›´å¤šå­—æ®µ")
        print("=" * 70)
        
        await self.init()
        
        for city in CITIES[:2]:  # å…ˆæµ‹è¯•2ä¸ªåŸå¸‚
            videos = await self.crawl_city(city)
            self.results.extend(videos)
            await asyncio.sleep(3)
        
        if self.results:
            self.save_to_db(self.results)
        
        await self.close()
        
        print("\n" + "=" * 70)
        print(f"âœ… å®Œæˆ: {len(self.results)} æ¡è§†é¢‘")
        for city in CITIES[:2]:
            count = sum(1 for v in self.results if v.city == city)
            print(f"   {city}: {count} æ¡")
        print("=" * 70)


async def main():
    crawler = EnhancedVideoCrawler()
    await crawler.run()


if __name__ == '__main__':
    asyncio.run(main())
