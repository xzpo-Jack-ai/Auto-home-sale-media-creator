#!/usr/bin/env python3
"""
æŠ–éŸ³æˆ¿äº§æ•°æ®æŠ“å–è„šæœ¬
åŠŸèƒ½ï¼šæ¯å¤©è‡ªåŠ¨æŠ“å– 6 ä¸ªåŸå¸‚çš„æˆ¿äº§çƒ­è¯å’Œçƒ­é—¨è§†é¢‘
ä½œè€…ï¼šShadowJack
æ—¥æœŸï¼š2026-02-28
"""

import asyncio
import json
import re
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from urllib.parse import quote

from playwright.async_api import async_playwright, Page, BrowserContext

# ============ é…ç½® ============
CITIES = ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'å¹¿å·', 'æ­å·', 'æˆéƒ½']

# Cookie å­—ç¬¦ä¸²ï¼ˆä»ç”¨æˆ·è·å–ï¼‰
COOKIE_STRING = """gfkadpd=2906,33638;is_staff_user=false;sessionid_ss=0a6e62e78eb34e330971d20ec2818ade;passport_csrf_token=c3a6730d181d2559d7ba0490d6c40ff9;sid_ucp_v1=1.0.0-KDcyN2Y0NzkzNmY5OTA3NmMzMzk3MTE1YjZmZjUyMWZjOTYyMWYwZjQKHwik-8O4mgIQkZ7_zAYY7zEgDDCTpKnQBTgHQPQHSAQaAmxxIiAwYTZlNjJlNzhlYjM0ZTMzMDk3MWQyMGVjMjgxOGFkZQ;session_tlb_tag=sttt%7C17%7CCm5i546zTjMJcdIOwoGK3v_________m1ImgMU1rqmkxDJ9b9Eh0z3EF9oWUKB9qkCADhjqPaSs%3D;passport_mfa_token=CjW01bYM3U0UthUSqMagXEC5czOIWCHWyp3phW2v5zQRMU4PttqioSqYcjiWW6FGipmsYjnLHBpKCjwAAAAAAAAAAAAAUB3AYXd0Ytp4C84uhbNzuHJOqN%2FExi0w6%2BK9eXQAcz7bgEZd7cW5UVwJI0LFGmHRs64Qr9GKDhj2sdFsIAIiAQNZMK3k;sid_guard=0a6e62e78eb34e330971d20ec2818ade%7C1772080913%7C5184000%7CMon%2C+27-Apr-2026+04%3A41%3A53+GMT;ttwid=1%7CuAVNzXBkGVl22a2UT7kvfDmweeWtVRuGqJ9plwBVYmw%7C1772216039%7Cfdf445ceb5d2c533ce5a5ded1e054e376be994f8a43a47478fdc9927cee0a6d8;count-client-api_sid=eyJfZXhwaXJlIjoxNzczNDI1NjQwMjQ1LCJfbWF4QWdlIjoxMjA5NjAwMDAwfQ==;csrf_session_id=c8dde97ff722650b6430040530222c71;enter_pc_once=1;passport_assist_user=CjyXWaCwYmxVv7oLpozkXocMWVV8tuRhS4RVwQBtKudsy5trnjbVhBXft3_u4gGveQ6h35uPyeavTpQcVyQaSgo8AAAAAAAAAAAAAFAeuyhDZ5389LY5gMoIEZLLJaaUV6FDgrGRwf0spalY576rMiDST20Oaw1PVta3xntLENfTig4Yia_WVCABIgEDoMFlfw%3D%3D;sessionid=0a6e62e78eb34e330971d20ec2818ade;sid_tt=0a6e62e78eb34e330971d20ec2818ade;ssid_ucp_v1=1.0.0-KDcyN2Y0NzkzNmY5OTA3NmMzMzk3MTE1YjZmZjUyMWZjOTYyMWYwZjQKHwik-8O4mgIQkZ7_zAYY7zEgDDCTpKnQBTgHQPQHSAQaAmxxIiAwYTZlNjJlNzhlYjM0ZTMzMDk3MWQyMGVjMjgxOGFkZQ;uid_tt=6cda6e3e53db8a3cae8f1ff47c9fe91a;uid_tt_ss=6cda6e3e53db8a3cae8f1ff47c9fe91a;UIFID_TEMP=749b770aa6a177ba6fbed42b6fcf8269d6ef3c63265bceaf64e3282dcaa6c732bcd33b0d6b597c0e89d8d155e604602697c77de025655c522075eb9618fa3c22e0ed9c812eefb093400b670094570fe5b1d6603a7e338f98530942c2f5aa521e02b228ab2ee7e8daa8a6da9f74deb10a"""

# æ•°æ®åº“è·¯å¾„
DB_PATH = Path("/Volumes/movespace/workspace/Auto-home-sale-media-creator/apps/api/dev.db")


@dataclass
class HotKeyword:
    """çƒ­è¯æ•°æ®æ¨¡å‹"""
    city: str
    keyword: str
    heat_value: int
    trend: str  # 'up', 'down', 'stable'
    rank: int
    crawled_at: datetime


@dataclass
class VideoData:
    """è§†é¢‘æ•°æ®æ¨¡å‹"""
    city: str
    keyword: str
    title: str
    author: str
    views: int
    likes: int
    shares: int
    comments: int
    link: str
    cover_url: str
    duration: int
    published_at: Optional[datetime]
    crawled_at: datetime


class DouyinCrawler:
    """æŠ–éŸ³æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.context: Optional[BrowserContext] = None
        self.cookies = self._parse_cookies()
        self.results = {
            'keywords': [],
            'videos': [],
            'errors': []
        }
    
    def _parse_cookies(self) -> List[Dict[str, str]]:
        """è§£æ Cookie å­—ç¬¦ä¸²"""
        cookies = []
        for item in COOKIE_STRING.split(';'):
            item = item.strip()
            if '=' in item:
                name, value = item.split('=', 1)
                cookies.append({
                    'name': name,
                    'value': value,
                    'domain': '.douyin.com',
                    'path': '/'
                })
        return cookies
    
    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        # æ·»åŠ  Cookie
        await self.context.add_cookies(self.cookies)
        print(f"âœ… æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½ {len(self.cookies)} ä¸ª Cookie")
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            await self.context.close()
        if hasattr(self, 'browser'):
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def fetch_city_hot_keywords(self, city: str) -> List[HotKeyword]:
        """æŠ“å–åŸå¸‚æˆ¿äº§çƒ­è¯"""
        keywords = []
        search_query = f"{city}æˆ¿äº§"
        
        try:
            page = await self.context.new_page()
            
            # è®¿é—®å·¨é‡ç®—æ•°æœç´¢é¡µ
            url = f"https://trendinsight.oceanengine.com/arithmetic-index/analysis?keyword={quote(search_query)}"
            print(f"ğŸ” æ­£åœ¨æŠ“å– [{city}] çƒ­è¯: {url}")
            
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            if response.status != 200:
                raise Exception(f"é¡µé¢åŠ è½½å¤±è´¥: {response.status}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(3)
            
            # å°è¯•æå–ç›¸å…³çƒ­è¯
            # æ³¨æ„ï¼šå®é™…é€‰æ‹©å™¨éœ€è¦æ ¹æ®é¡µé¢ç»“æ„è°ƒæ•´
            hot_words = await page.evaluate('''() => {
                const words = [];
                // å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
                const selectors = [
                    '.related-word-item',
                    '.hot-word-item',
                    '[class*="word"]',
                    '[class*="keyword"]'
                ];
                
                for (const selector of selectors) {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach(el => {
                        const text = el.textContent?.trim();
                        if (text && text.length > 2 && text.length < 50) {
                            words.push(text);
                        }
                    });
                }
                
                return [...new Set(words)].slice(0, 10);
            }''')
            
            print(f"   æ‰¾åˆ° {len(hot_words)} ä¸ªçƒ­è¯: {hot_words[:5]}...")
            
            # æ„é€ çƒ­è¯å¯¹è±¡
            for i, word in enumerate(hot_words[:10], 1):
                keywords.append(HotKeyword(
                    city=city,
                    keyword=word,
                    heat_value=100 - i * 5,  # æ¨¡æ‹Ÿçƒ­åº¦å€¼
                    trend='up' if i % 3 == 0 else 'stable',
                    rank=i,
                    crawled_at=datetime.now()
                ))
            
            await page.close()
            
        except Exception as e:
            error_msg = f"[{city}] çƒ­è¯æŠ“å–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            self.results['errors'].append(error_msg)
        
        return keywords
    
    async def fetch_city_videos(self, city: str, keyword: str) -> List[VideoData]:
        """æŠ“å–åŸå¸‚å…³é”®è¯ç›¸å…³è§†é¢‘"""
        videos = []
        search_query = f"{city}{keyword}"
        
        try:
            page = await self.context.new_page()
            
            # è®¿é—®åˆ›ä½œè€…å¹³å°è§†é¢‘æœç´¢
            url = f"https://creator.douyin.com/creator-micro/creator-count/arithmetic-index/videosearch?query={quote(search_query)}"
            print(f"ğŸ¬ æ­£åœ¨æŠ“å– [{city}-{keyword}] è§†é¢‘: {url}")
            
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            if response.status != 200:
                raise Exception(f"é¡µé¢åŠ è½½å¤±è´¥: {response.status}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(3)
            
            # å°è¯•æå–è§†é¢‘ä¿¡æ¯
            video_list = await page.evaluate('''() => {
                const videos = [];
                const selectors = [
                    '.video-item',
                    '[class*="video"]',
                    '[class*="card"]'
                ];
                
                for (const selector of selectors) {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach(el => {
                        const titleEl = el.querySelector('[class*="title"]') || el.querySelector('h3, h4');
                        const authorEl = el.querySelector('[class*="author"]') || el.querySelector('[class*="user"]');
                        const viewEl = el.querySelector('[class*="view"]') || el.querySelector('[class*="play"]');
                        
                        if (titleEl) {
                            videos.push({
                                title: titleEl.textContent?.trim() || '',
                                author: authorEl?.textContent?.trim() || 'æœªçŸ¥ä½œè€…',
                                views: viewEl?.textContent?.trim() || '0'
                            });
                        }
                    });
                }
                
                return videos.slice(0, 5);
            }''')
            
            print(f"   æ‰¾åˆ° {len(video_list)} ä¸ªè§†é¢‘")
            
            # æ„é€ è§†é¢‘å¯¹è±¡
            for i, v in enumerate(video_list):
                # è§£ææ’­æ”¾é‡æ•°å­—
                views_str = v.get('views', '0')
                views_num = self._parse_number(views_str)
                
                videos.append(VideoData(
                    city=city,
                    keyword=keyword,
                    title=v.get('title', 'æ— æ ‡é¢˜'),
                    author=v.get('author', 'æœªçŸ¥ä½œè€…'),
                    views=views_num,
                    likes=int(views_num * 0.05),  # ä¼°ç®—
                    shares=int(views_num * 0.01),
                    comments=int(views_num * 0.02),
                    link='',  # éœ€è¦è¿›ä¸€æ­¥æå–
                    cover_url='',
                    duration=30 + i * 10,
                    published_at=datetime.now() - timedelta(days=i),
                    crawled_at=datetime.now()
                ))
            
            await page.close()
            
        except Exception as e:
            error_msg = f"[{city}-{keyword}] è§†é¢‘æŠ“å–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            self.results['errors'].append(error_msg)
        
        return videos
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—ï¼ˆæ”¯æŒä¸‡ã€äº¿ç­‰å•ä½ï¼‰"""
        text = text.replace(',', '').strip()
        
        if 'ä¸‡' in text:
            num = float(text.replace('ä¸‡', ''))
            return int(num * 10000)
        elif 'äº¿' in text:
            num = float(text.replace('äº¿', ''))
            return int(num * 100000000)
        elif text.isdigit():
            return int(text)
        else:
            # å°è¯•æå–æ•°å­—
            nums = re.findall(r'\d+', text)
            return int(nums[0]) if nums else 0
    
    def save_to_database(self):
        """ä¿å­˜æ•°æ®åˆ° SQLite æ•°æ®åº“"""
        if not DB_PATH.exists():
            print(f"âš ï¸ æ•°æ®åº“ä¸å­˜åœ¨: {DB_PATH}")
            return False
        
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            # æ›´æ–°çƒ­è¯æ•°æ®
            for kw in self.results['keywords']:
                cursor.execute('''
                    INSERT INTO Keyword (city, text, heat, updatedAt)
                    VALUES (?, ?, ?, ?)
                    ON CONFLICT(city, text) DO UPDATE SET
                        heat = excluded.heat,
                        updatedAt = excluded.updatedAt
                ''', (kw.city, kw.keyword, kw.heat_value, kw.crawled_at.isoformat()))
            
            # æ’å…¥è§†é¢‘æ•°æ®
            for v in self.results['videos']:
                external_id = f"{v.city}_{v.keyword}_{v.title[:20]}_{int(v.crawled_at.timestamp())}"
                cursor.execute('''
                    INSERT OR REPLACE INTO Video (
                        externalId, platform, title, author, authorId,
                        views, likes, shares, comments, coverUrl, duration,
                        transcript, publishedAt, keyword, city, createdAt
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    external_id, 'douyin', v.title, v.author, '',
                    v.views, v.likes, v.shares, v.comments, v.cover_url, v.duration,
                    '', v.published_at.isoformat() if v.published_at else None,
                    v.keyword, v.city, v.crawled_at.isoformat()
                ))
            
            conn.commit()
            conn.close()
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
            print(f"   çƒ­è¯: {len(self.results['keywords'])} æ¡")
            print(f"   è§†é¢‘: {len(self.results['videos'])} æ¡")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    async def run(self, test_mode: bool = False):
        """è¿è¡ŒæŠ“å–ä»»åŠ¡"""
        print("=" * 60)
        print("ğŸš€ æŠ–éŸ³æˆ¿äº§æ•°æ®æŠ“å–ä»»åŠ¡å¼€å§‹")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"åŸå¸‚: {', '.join(CITIES)}")
        print(f"æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼' if test_mode else 'å®Œæ•´æ¨¡å¼'}")
        print("-" * 60)
        
        try:
            await self.init_browser()
            
            # æµ‹è¯•æ¨¡å¼åªæŠ“ä¸€ä¸ªåŸå¸‚
            cities_to_crawl = CITIES[:1] if test_mode else CITIES
            
            for city in cities_to_crawl:
                print(f"\nğŸ“ æ­£åœ¨å¤„ç†åŸå¸‚: {city}")
                
                # 1. æŠ“å–çƒ­è¯
                keywords = await self.fetch_city_hot_keywords(city)
                self.results['keywords'].extend(keywords)
                
                # 2. ä¸ºæ¯ä¸ªçƒ­è¯æŠ“å–è§†é¢‘ï¼ˆæµ‹è¯•æ¨¡å¼åªæŠ“å‰2ä¸ªçƒ­è¯ï¼‰
                keywords_to_process = keywords[:2] if test_mode else keywords[:5]
                for kw in keywords_to_process:
                    videos = await self.fetch_city_videos(city, kw.keyword)
                    self.results['videos'].extend(videos)
                    await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
                await asyncio.sleep(2)  # åŸå¸‚é—´é—´éš”
            
            # ä¿å­˜æ•°æ®
            self.save_to_database()
            
        finally:
            await self.close()
        
        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š æŠ“å–ä»»åŠ¡å®Œæˆ")
        print("=" * 60)
        print(f"æˆåŠŸåŸå¸‚: {len(set(kw.city for kw in self.results['keywords']))}/{len(cities_to_crawl)}")
        print(f"çƒ­è¯æ€»æ•°: {len(self.results['keywords'])}")
        print(f"è§†é¢‘æ€»æ•°: {len(self.results['videos'])}")
        print(f"é”™è¯¯æ•°é‡: {len(self.results['errors'])}")
        
        if self.results['errors']:
            print("\nâš ï¸ é”™è¯¯è¯¦æƒ…:")
            for err in self.results['errors'][:5]:
                print(f"   â€¢ {err}")
        
        return len(self.results['errors']) == 0


async def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥å‚æ•°
    test_mode = '--test' in sys.argv
    
    crawler = DouyinCrawler()
    success = await crawler.run(test_mode=test_mode)
    
    return 0 if success else 1


if __name__ == '__main__':
    exit_code = asyncio.run(main())
    exit(exit_code)
