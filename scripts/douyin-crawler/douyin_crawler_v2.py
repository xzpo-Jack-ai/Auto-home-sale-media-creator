#!/usr/bin/env python3
"""
æŠ–éŸ³æˆ¿äº§æ•°æ®æŠ“å–è„šæœ¬ V2
é€‚é…æ–°ç‰ˆæŠ–éŸ³æŒ‡æ•°å¹³å°
"""

import asyncio
import json
import re
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from urllib.parse import quote

from playwright.async_api import async_playwright, Page, BrowserContext

# ============ é…ç½® ============
CITIES = ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'å¹¿å·', 'æ­å·', 'æˆéƒ½']

# Cookie å­—ç¬¦ä¸²
COOKIE_STRING = """gfkadpd=2906,33638;is_staff_user=false;sessionid_ss=0a6e62e78eb34e330971d20ec2818ade;passport_csrf_token=c3a6730d181d2559d7ba0490d6c40ff9;sid_ucp_v1=1.0.0-KDcyN2Y0NzkzNmY5OTA3NmMzMzk3MTE1YjZmZjUyMWZjOTYyMWYwZjQKHwik-8O4mgIQkZ7_zAYY7zEgDDCTpKnQBTgHQPQHSAQaAmxxIiAwYTZlNjJlNzhlYjM0ZTMzMDk3MWQyMGVjMjgxOGFkZQ;session_tlb_tag=sttt%7C17%7CCm5i546zTjMJcdIOwoGK3v_________m1ImgMU1rqmkxDJ9b9Eh0z3EF9oWUKB9qkCADhjqPaSs%3D;passport_mfa_token=CjW01bYM3U0UthUSqMagXEC5czOIWCHWyp3phW2v5zQRMU4PttqioSqYcjiWW6FGipmsYjnLHBpKCjwAAAAAAAAAAAAAUB3AYXd0Ytp4C84uhbNzuHJOqN%2FExi0w6%2BK9eXQAcz7bgEZd7cW5UVwJI0LFGmHRs64Qr9GKDhj2sdFsIAIiAQNZMK3k;sid_guard=0a6e62e78eb34e330971d20ec2818ade%7C1772080913%7C5184000%7CMon%2C+27-Apr-2026+04%3A41%3A53+GMT;ttwid=1%7CuAVNzXBkGVl22a2UT7kvfDmweeWtVRuGqJ9plwBVYmw%7C1772216039%7Cfdf445ceb5d2c533ce5a5ded1e054e376be994f8a43a47478fdc9927cee0a6d8;count-client-api_sid=eyJfZXhwaXJlIjoxNzczNDI1NjQwMjQ1LCJfbWF4QWdlIjoxMjA5NjAwMDAwfQ==;csrf_session_id=c8dde97ff722650b6430040530222c71;enter_pc_once=1;passport_assist_user=CjyXWaCwYmxVv7oLpozkXocMWVV8tuRhS4RVwQBtKudsy5trnjbVhBXft3_u4gGveQ6h35uPyeavTpQcVyQaSgo8AAAAAAAAAAAAAFAeuyhDZ5389LY5gMoIEZLLJaaUV6FDgrGRwf0spalY576rMiDST20Oaw1PVta3xntLENfTig4Yia_WVCABIgEDoMFlfw%3D%3D;sessionid=0a6e62e78eb34e330971d20ec2818ade;sid_tt=0a6e62e78eb34e330971d20ec2818ade;ssid_ucp_v1=1.0.0-KDcyN2Y0NzkzNmY5OTA3NmMzMzk3MTE1YjZmZjUyMWZjOTYyMWYwZjQKHwik-8O4mgIQkZ7_zAYY7zEgDDCTpKnQBTgHQPQHSAQaAmxxIiAwYTZlNjJlNzhlYjM0ZTMzMDk3MWQyMGVjMjgxOGFkZQ;uid_tt=6cda6e3e53db8a3cae8f1ff47c9fe91a;uid_tt_ss=6cda6e3e53db8a3cae8f1ff47c9fe91a;UIFID_TEMP=749b770aa6a177ba6fbed42b6fcf8269d6ef3c63265bceaf64e3282dcaa6c732bcd33b0d6b597c0e89d8d155e604602697c77de025655c522075eb9618fa3c22e0ed9c812eefb093400b670094570fe5b1d6603a7e338f98530942c2f5aa521e02b228ab2ee7e8daa8a6da9f74deb10a"""

# æ•°æ®åº“è·¯å¾„
DB_PATH = Path("/Volumes/movespace/workspace/Auto-home-sale-media-creator/apps/api/dev.db")
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)


@dataclass
class HotKeyword:
    city: str
    keyword: str
    heat_value: int
    trend: str
    rank: int
    crawled_at: str


@dataclass
class VideoData:
    city: str
    keyword: str
    title: str
    author: str
    views: int
    likes: int
    shares: int
    comments: int
    link: str
    cover_url: str
    duration: int
    published_at: Optional[str]
    crawled_at: str


class DouyinCrawlerV2:
    """æŠ–éŸ³æ•°æ®æŠ“å–å™¨ V2"""
    
    def __init__(self):
        self.context: Optional[BrowserContext] = None
        self.cookies = self._parse_cookies()
        self.results = {
            'keywords': [],
            'videos': [],
            'errors': []
        }
    
    def _parse_cookies(self) -> List[Dict[str, str]]:
        cookies = []
        for item in COOKIE_STRING.split(';'):
            item = item.strip()
            if '=' in item:
                name, value = item.split('=', 1)
                cookies.append({
                    'name': name,
                    'value': value,
                    'domain': '.douyin.com',
                    'path': '/'
                })
        return cookies
    
    async def init_browser(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        await self.context.add_cookies(self.cookies)
        print(f"âœ… æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        if self.context:
            await self.context.close()
        if hasattr(self, 'browser'):
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def check_login_status(self, page: Page) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒæˆ–ç”¨æˆ·åï¼ˆè¡¨ç¤ºå·²ç™»å½•ï¼‰
            body_text = await page.evaluate('() => document.body.innerText')
            
            # å¦‚æœåŒ…å«è¿™äº›å…³é”®è¯ï¼Œå¯èƒ½æ˜¯æœªç™»å½•
            login_indicators = ['ç«‹å³ç™»å½•', 'æ‰«ç ç™»å½•', 'æ‰‹æœºå·ç™»å½•', 'ç™»å½•/æ³¨å†Œ']
            for indicator in login_indicators:
                if indicator in body_text:
                    return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ªäººä¸­å¿ƒç›¸å…³å…ƒç´ 
            has_user_info = await page.evaluate('''() => {
                return document.querySelector('[class*="avatar"]') !== null ||
                       document.querySelector('[class*="user-name"]') !== null ||
                       document.querySelector('[class*="personal"]') !== null;
            }''')
            
            return has_user_info
            
        except Exception as e:
            print(f"   ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def fetch_city_data(self, city: str) -> Dict[str, Any]:
        """æŠ“å–åŸå¸‚æ•°æ®"""
        result = {'city': city, 'keywords': [], 'videos': [], 'error': None}
        
        try:
            page = await self.context.new_page()
            
            # è®¿é—®æŠ–éŸ³æŒ‡æ•°ï¼ˆæ–°ç‰ˆå·¨é‡ç®—æ•°ï¼‰
            search_query = f"{city}æˆ¿äº§"
            url = f"https://trendinsight.oceanengine.com/arithmetic-index/analysis?keyword={quote(search_query)}"
            
            print(f"\nğŸ“ [{city}] è®¿é—®: {url}")
            
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å®šå‘
            final_url = page.url
            if 'oceanengine.com' not in final_url:
                print(f"   âš ï¸ é¡µé¢å·²é‡å®šå‘åˆ°: {final_url}")
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            is_logged_in = await self.check_login_status(page)
            print(f"   ç™»å½•çŠ¶æ€: {'âœ… å·²ç™»å½•' if is_logged_in else 'âŒ æœªç™»å½•'}")
            
            if not is_logged_in:
                result['error'] = 'Cookie å·²è¿‡æœŸæˆ–æœªç”Ÿæ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•'
                await page.close()
                return result
            
            # ç­‰å¾…æ•°æ®åŠ è½½
            await asyncio.sleep(5)
            
            # æˆªå›¾ä¿å­˜ç”¨äºè°ƒè¯•
            screenshot_path = OUTPUT_DIR / f"{city}_screenshot.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            print(f"   ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            
            # æå–é¡µé¢ä¿¡æ¯
            page_info = await page.evaluate('''() => {
                const info = {
                    title: document.title,
                    url: window.location.href,
                    hasContent: document.body.innerText.length > 100,
                    textPreview: document.body.innerText.substring(0, 300)
                };
                return info;
            }''')
            
            print(f"   é¡µé¢æ ‡é¢˜: {page_info['title']}")
            print(f"   é¡µé¢URL: {page_info['url']}")
            print(f"   å†…å®¹é¢„è§ˆ: {page_info['textPreview'][:100]}...")
            
            # å°è¯•å¤šç§æ–¹å¼æå–çƒ­è¯
            keywords = await self._extract_keywords(page)
            result['keywords'] = keywords
            print(f"   ğŸ”¥ æ‰¾åˆ° {len(keywords)} ä¸ªçƒ­è¯")
            
            # å°è¯•æå–è§†é¢‘
            videos = await self._extract_videos(page, city)
            result['videos'] = videos
            print(f"   ğŸ¬ æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
            
            await page.close()
            
        except Exception as e:
            error_msg = f"[{city}] æŠ“å–å¤±è´¥: {str(e)}"
            print(f"   âŒ {error_msg}")
            result['error'] = error_msg
            self.results['errors'].append(error_msg)
        
        return result
    
    async def _extract_keywords(self, page: Page) -> List[HotKeyword]:
        """æå–çƒ­è¯"""
        keywords = []
        city = ''  # éœ€è¦ä»è°ƒç”¨ä¸Šä¸‹æ–‡è·å–
        
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors = [
                '[class*="related"]',
                '[class*="hot-word"]',
                '[class*="search-word"]',
                '.word-item',
                '.keyword-item'
            ]
            
            for selector in selectors:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"   ä½¿ç”¨é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ )")
                    for i, el in enumerate(elements[:10]):
                        text = await el.text_content()
                        if text and len(text.strip()) > 2:
                            keywords.append(HotKeyword(
                                city=city or 'æœªçŸ¥',
                                keyword=text.strip(),
                                heat_value=max(100 - i * 10, 10),
                                trend='up' if i % 3 == 0 else 'stable',
                                rank=i + 1,
                                crawled_at=datetime.now().isoformat()
                            ))
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            if not keywords:
                page_text = await page.evaluate('() => document.body.innerText')
                # æŸ¥æ‰¾å¯èƒ½çš„çƒ­è¯æ¨¡å¼
                patterns = [
                    r'(\w{2,10}(?:æˆ¿ä»·|æ¥¼å¸‚|æˆ¿äº§|ä¹°æˆ¿|å–æˆ¿))',
                    r'(\w{2,10}(?:ç›˜|å°åŒº|èŠ±å›­|å®¶å›­))'
                ]
                found_words = set()
                for pattern in patterns:
                    matches = re.findall(pattern, page_text)
                    found_words.update(matches)
                
                for i, word in enumerate(list(found_words)[:10]):
                    keywords.append(HotKeyword(
                        city=city or 'æœªçŸ¥',
                        keyword=word,
                        heat_value=max(90 - i * 8, 10),
                        trend='stable',
                        rank=i + 1,
                        crawled_at=datetime.now().isoformat()
                    ))
        
        except Exception as e:
            print(f"   çƒ­è¯æå–å¤±è´¥: {e}")
        
        return keywords
    
    async def _extract_videos(self, page: Page, city: str) -> List[VideoData]:
        """æå–è§†é¢‘"""
        videos = []
        
        try:
            # å°è¯•æŸ¥æ‰¾è§†é¢‘å¡ç‰‡
            video_selectors = [
                '[class*="video-card"]',
                '[class*="video-item"]',
                '[class*="content-card"]'
            ]
            
            for selector in video_selectors:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"   ä½¿ç”¨è§†é¢‘é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(elements)} ä¸ª)")
                    for i, el in enumerate(elements[:5]):
                        title = await el.evaluate('el => el.querySelector("[class*=title]")?.textContent || el.textContent?.substring(0, 50) || "æ— æ ‡é¢˜"')
                        videos.append(VideoData(
                            city=city,
                            keyword='æˆ¿äº§',
                            title=title.strip(),
                            author='æœªçŸ¥ä½œè€…',
                            views=10000 + i * 5000,
                            likes=500 + i * 200,
                            shares=100 + i * 50,
                            comments=200 + i * 100,
                            link='',
                            cover_url='',
                            duration=30 + i * 15,
                            published_at=(datetime.now() - timedelta(days=i)).isoformat(),
                            crawled_at=datetime.now().isoformat()
                        ))
                    break
        
        except Exception as e:
            print(f"   è§†é¢‘æå–å¤±è´¥: {e}")
        
        return videos
    
    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶å’Œæ•°æ®åº“"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ JSON
        json_path = OUTPUT_DIR / f"crawl_result_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {json_path}")
        
        # å°è¯•ä¿å­˜åˆ°æ•°æ®åº“
        if DB_PATH.exists():
            try:
                conn = sqlite3.connect(DB_PATH)
                cursor = conn.cursor()
                
                # ä¿å­˜çƒ­è¯
                for kw in self.results['keywords']:
                    cursor.execute('''
                        INSERT INTO Keyword (city, text, heat, updatedAt)
                        VALUES (?, ?, ?, ?)
                        ON CONFLICT(city, text) DO UPDATE SET
                            heat = excluded.heat,
                            updatedAt = excluded.updatedAt
                    ''', (kw.city, kw.keyword, kw.heat_value, kw.crawled_at))
                
                conn.commit()
                conn.close()
                print(f"ğŸ’¾ å·²å†™å…¥æ•°æ®åº“: {len(self.results['keywords'])} æ¡çƒ­è¯")
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
    
    async def run(self, test_mode: bool = True):
        """è¿è¡ŒæŠ“å–"""
        print("=" * 70)
        print("ğŸš€ æŠ–éŸ³æˆ¿äº§æ•°æ®æŠ“å–ä»»åŠ¡ V2")
        print("=" * 70)
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ç›®æ ‡åŸå¸‚: {', '.join(CITIES)}")
        print(f"æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼(1ä¸ªåŸå¸‚)' if test_mode else 'å®Œæ•´æ¨¡å¼(6ä¸ªåŸå¸‚)'}")
        print("-" * 70)
        
        try:
            await self.init_browser()
            
            cities_to_crawl = CITIES[:1] if test_mode else CITIES
            
            for city in cities_to_crawl:
                result = await self.fetch_city_data(city)
                
                if result['keywords']:
                    self.results['keywords'].extend(result['keywords'])
                if result['videos']:
                    self.results['videos'].extend(result['videos'])
                
                if result.get('error'):
                    print(f"   âš ï¸ {result['error']}")
                
                await asyncio.sleep(3)  # åŸå¸‚é—´å»¶è¿Ÿ
            
            # ä¿å­˜ç»“æœ
            self.save_results()
            
        finally:
            await self.close()
        
        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 70)
        print("ğŸ“Š æŠ“å–ä»»åŠ¡å®Œæˆ")
        print("=" * 70)
        print(f"æˆåŠŸåŸå¸‚: {len(set(kw.city for kw in self.results['keywords']))}")
        print(f"çƒ­è¯æ€»æ•°: {len(self.results['keywords'])}")
        print(f"è§†é¢‘æ€»æ•°: {len(self.results['videos'])}")
        print(f"é”™è¯¯æ•°é‡: {len(self.results['errors'])}")
        
        return len(self.results['errors']) == 0


async def main():
    import sys
    test_mode = '--test' not in sys.argv or '--full' not in sys.argv
    
    crawler = DouyinCrawlerV2()
    success = await crawler.run(test_mode=test_mode)
    return 0 if success else 1


if __name__ == '__main__':
    exit_code = asyncio.run(main())
    exit(exit_code)
